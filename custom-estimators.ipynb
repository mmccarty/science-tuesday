{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "embedded-airport",
   "metadata": {},
   "source": [
    "# Custom Estimators on Dask\n",
    "\n",
    "Code examples taken from [blog post](https://www.capitalone.com/tech/machine-learning/custom-machine-learning-estimators-on-dask-and-rapids/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-understanding",
   "metadata": {},
   "source": [
    "## Scikit-learn example\n",
    "\n",
    "From the scikit-learn [documentation](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html#pipelining) for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "pca = PCA()\n",
    "# set the tolerance to a large value to make the example faster\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': [5, 15, 30, 45, 64],\n",
    "    'logistic__C': np.logspace(-4, 4, 4),\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=123)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best = search.best_estimator_\n",
    "\n",
    "print(f\"Training set score: {best.score(X_train, y_train)}\")\n",
    "print(f\"Test set score: {best.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-ideal",
   "metadata": {},
   "source": [
    "## Option 1 for Customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(X):\n",
    "    \"\"\"Mutates X\"\"\"\n",
    "    # ... do something ...\n",
    "    return X\n",
    "\n",
    "pca = PCA(n_components=search.best_params_['pca__n_components'])\n",
    "logistic = LogisticRegression(\n",
    "    max_iter=10000, tol=0.1, C=search.best_params_['logistic__C'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_digits, y_digits, random_state=123)\n",
    "\n",
    "X_train = pca.fit_transform(X_train, y_train)\n",
    "X_train = mutate(X_train)\n",
    "logistic = logistic.fit(X_train, y_train)\n",
    "\n",
    "X_test = pca.transform(X_test) # <- Don't call fit again!\n",
    "X_test = mutate(X_test) # <-Don’t forget to call mutate on X_test!\n",
    "\n",
    "print(f\"Training set score: {logistic.score(X_train, y_train)}\")\n",
    "print(f\"Test set score: {logistic.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-international",
   "metadata": {},
   "source": [
    "## Option 2 for Customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from abc import ABCMeta\n",
    "\n",
    "class Mutate(TransformerMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Mutates X\"\"\"\n",
    "        # ... do something ...\n",
    "        return X\n",
    "\n",
    "pca = PCA(n_components=search.best_params_['pca__n_components'])\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1, C=search.best_params_['logistic__C'])\n",
    "\n",
    "pipe = Pipeline(steps=[('pca', pca), ('mutate', Mutate()), ('logistic', logistic)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=123)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(f\"Training set score: {pipe.score(X_train, y_train)}\")\n",
    "print(f\"Test set score: {pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-swiss",
   "metadata": {},
   "source": [
    "# Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('exp a')\n",
    "\n",
    "class CustomSearchCV(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, estimator, cv, logger):\n",
    "        self.estimator = estimator\n",
    "        self.cv = cv\n",
    "        \n",
    "        self.logger = logger\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y=None, **fit_kws):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        # Insert more guards here!\n",
    "            \n",
    "        X_base, X_holdout, y_base, y_holdout = train_test_split(\n",
    "            X, y, random_state=123)\n",
    "        \n",
    "        self.split_scores_ = []\n",
    "        self.holdout_scores_ = []\n",
    "        self.estimators_ = []            \n",
    "        \n",
    "        for train_idx, test_idx in self.cv.split(X_base, y_base):\n",
    "            X_test, y_test = X_base[test_idx], y_base[test_idx]\n",
    "            X_train, y_train = X_base[train_idx], y_base[train_idx]\n",
    "\n",
    "            estimator_ = clone(self.estimator)\n",
    "            estimator_.fit(X_train, y_train, **fit_kws)\n",
    "\n",
    "            self.logger.info(\"... log things ...\")\n",
    "            self.estimators_.append(estimator_)\n",
    "            self.split_scores_.append(estimator_.score(X_test, y_test))            \n",
    "            self.holdout_scores_.append(\n",
    "                estimator_.score(X_holdout, y_holdout))\n",
    "    \n",
    "        self.best_estimator_ = \\\n",
    "                self.estimators_[np.argmax(self.holdout_scores_)]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=100_000,\n",
    "    n_features=100,\n",
    "    weights=[0.75, 0.25],\n",
    "    flip_y=0.75,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "cv = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n",
    "clf = CustomSearchCV(xgb.XGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), cv, logger)\n",
    "\n",
    "clf.fit(X, y)\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-campaign",
   "metadata": {},
   "source": [
    "# Scale with Dask on Coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a Dask cluster with Coiled. To reproduce this\n",
    "# with coiled you'd need to\n",
    "# - visit https://coiled.io to sign up for an account.\n",
    "# - Do their initial setup (login, create an environment)\n",
    "# - change `account=\"dask\"` to your account.\n",
    "# Alternatively, you can deploy Dask on your own using one\n",
    "# of the methods at https://docs.dask.org/en/latest/setup.html.\n",
    "\n",
    "import coiled\n",
    "cluster = coiled.Cluster(n_workers=20, software=\"custom-estimators-env\")\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from dask.base import is_dask_collection\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('exp a')\n",
    "\n",
    "class CustomSearchCV(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, estimator, cv, logger):\n",
    "        self.estimator = estimator\n",
    "        self.cv = cv\n",
    "        \n",
    "        self.logger = logger\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y=None, **fit_kws):\n",
    "        if isinstance(X, dd.DataFrame):\n",
    "            X = X.to_dask_array(lengths=True)\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if is_dask_collection(X):\n",
    "            print(\"dask\")\n",
    "            from dask_ml.model_selection import train_test_split\n",
    "        else:\n",
    "            from sklearn.model_selection import train_test_split\n",
    "        # Insert more guards here!\n",
    "            \n",
    "        X_base, X_holdout, y_base, y_holdout = train_test_split(\n",
    "            X, y, random_state=123)\n",
    "        \n",
    "        self.split_scores_ = []\n",
    "        self.holdout_scores_ = []\n",
    "        self.estimators_ = []            \n",
    "        \n",
    "        for train_idx, test_idx in self.cv.split(X_base, y_base):\n",
    "            X_test, y_test = X_base[test_idx], y_base[test_idx]\n",
    "            X_train, y_train = X_base[train_idx], y_base[train_idx]\n",
    "\n",
    "            estimator_ = clone(self.estimator)\n",
    "            estimator_.fit(X_train, y_train, **fit_kws)\n",
    "\n",
    "            self.logger.info(\"... log things ...\")\n",
    "            self.estimators_.append(estimator_)\n",
    "            self.split_scores_.append(estimator_.score(X_test, y_test))            \n",
    "            self.holdout_scores_.append(\n",
    "                estimator_.score(X_holdout, y_holdout))\n",
    "    \n",
    "        self.best_estimator_ = \\\n",
    "                self.estimators_[np.argmax(self.holdout_scores_)]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "\n",
    "from dask_ml.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1_000_000,\n",
    "    n_features=100,\n",
    "    weights=[0.75, 0.25],\n",
    "    flip_y=0.75,\n",
    "    random_state=123,\n",
    "    chunks=100000\n",
    ")\n",
    "\n",
    "cv = KFold(n_splits=2, random_state=2652124)\n",
    "est = xgb.dask.DaskXGBClassifier(eval_metric='error', use_label_encoder=False)\n",
    "est.client = client\n",
    "clf = CustomSearchCV(est, cv, logger)\n",
    "\n",
    "clf.fit(X, y)\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-israeli",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
